{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Backpropagation by Hand\n",
    "The API for this MLP network was inspired by sklearn.  \n",
    "Derivations of update rules included but the network itself has a low accuracy for some reason, even with randomized weights, different activation functions etc.  \n",
    "Only the Kullbach-Liebler divergence was attempted, and unfortunately, although the derivations seem accurate (matrix dimensions line up), the gradient is too small to move the weights. It is not known why.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Import training data and labels \"\"\"\n",
    "train_data = np.genfromtxt('./train_data.csv', delimiter=',')\n",
    "train_labels = np.genfromtxt('./train_labels.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Activation Functions\n",
    "We include the following activation functions that can be used with the network.\n",
    "- Sigmoid $S(x)=\\frac{e^{x}}{1+e^{x}}$: a smooth and therefore differentiable alternative to the Signum function.\n",
    "\n",
    "- Tanh $tanh(x)$: hyperbolic tan, similar to the sigmoid function but steeper around 0.\n",
    "\n",
    "- ReLU $relu(x)=max\\{0, x\\}$: a non-linear ramp function\n",
    "\n",
    "Also, the derivatives of these function are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Activation functions \"\"\"\n",
    "\n",
    "def sigmoid(x): # sigmoid activation\n",
    "    \"\"\" sigmoid function \"\"\"\n",
    "    return 1/(np.exp(-x)+1)\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "def tanh(x): # tanh activation\n",
    "    \"\"\" tanh function\"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "def sec2h(x): # derivative of tanh\n",
    "    cosh2x = np.power(np.cosh(x), 2)\n",
    "    return 1/cosh2x\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return(np.heaviside(x, 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Gradient Calculation Functions\n",
    "Contains the evaluations of the derivatives necessary to compute the gradient of the weights.\n",
    "Our network with activation function $\\mathcal{F}$ will output the vector $$\\vec{y}^{pred}=\\mathcal{F}\\bigg(\\hat{W}^{O}\\mathcal{F}\\bigg(\\hat{W}^{H}\\vec{x}\\bigg)\\bigg)$$\n",
    "Using a loss function $\\mathcal{L}(\\vec{y}^{pred}, \\vec{y}_{target})$ we want to calculate the gradient with respect to the weights in the layer $i$ $\\frac{\\partial \\mathcal{L}}{\\partial \\hat{W}^{i}}$ and then adjust the weights in this direction.  \n",
    "Note: matrix calculus identities retrieved from here https://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf\n",
    "\n",
    "Since $$\\vec{y}^{pred}=\\mathcal{F}(\\vec{u})=\\mathcal{F}\\bigg(\\hat{W}^{O}\\vec{O}^{H}\\bigg)=\\mathcal{F}\\bigg(\\hat{W}^{O}\\mathcal{F}\\big(\\vec{v}\\big)\\bigg)=\\mathcal{F}\\bigg(\\hat{W}^{O}\\mathcal{F}\\bigg(\\hat{W}^{H}\\vec{x}\\bigg)\\bigg)$$ we arrive at the following chain rule formulae for the gradients\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\hat{W}^{H}}=\\frac{\\partial \\mathcal{L}}{\\partial \\vec{O}^H}\\frac{\\partial \\vec{O}^H}{\\partial \\hat{W}^H}=\\bigg(\\frac{\\partial \\mathcal{L}}{\\partial \\vec{O}^H}\\bigg)^T\\big(\\vec{x}\\big)^T=\\big(\\vec{\\delta_1} \\hat{\\delta_2} \\hat{\\delta_3}\\big)^T\\big(\\vec{x}\\big)^T$$  \n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\hat{W}^{O}}=\\frac{\\partial \\mathcal{L}}{\\partial \\vec{O}^O}\\frac{\\partial \\vec{O}^O}{\\partial \\hat{W}^O}=\\bigg(\\frac{\\partial \\mathcal{L}}{\\partial \\vec{O}^O}\\bigg)^T\\big(\\vec{O}^H\\big)^T=\\big(\\vec{\\delta_1} \\hat{\\delta_2}\\big)^T\\bigg(\\vec{O}^H\\bigg)^T$$\n",
    "where we have introduced\n",
    "$$\\vec{\\delta_1}=\\frac{\\partial \\mathcal{L}}{\\partial \\vec{y}^{pred}}$$  \n",
    "$$\\hat{\\delta_2}=\\frac{\\partial \\vec{y}^{pred}}{\\partial \\vec{u}}=diag\\bigg(\\mathcal{F\\prime}\\bigg({\\hat{W}^O\\vec{O}^H}\\bigg)\\bigg)$$  \n",
    "$$\\hat{\\delta_3}=\\frac{\\partial \\vec{u}}{\\partial \\vec{O}^H}=\\hat{W}^O$$\n",
    "\n",
    "## Sigmoid Derivative\n",
    "Therefore, for a sigmoid function:\n",
    "$$\\hat{\\delta_2}=diag\\bigg(S\\bigg(\\hat{W}^O\\vec{O}^H\\bigg)\\big(1-S\\bigg(\\hat{W}^O\\vec{O}^H\\bigg)\\big)\\bigg)$$  \n",
    "\n",
    "## Tanh Derivative\n",
    "for the tanh function:\n",
    "$$\\hat{\\delta_2}=diag\\bigg(sech^2\\big(\\hat{W}^O\\vec{O}^H\\big)\\bigg)$$\n",
    "\n",
    "To calculate $\\vec{\\delta_1}$ we need to consider the loss function we are using:\n",
    "\n",
    "### Kullbach-Liebler Divergence\n",
    "The Kullbach-Liebler Divergence of two probability distributions $P$ and $Q$ over a sample space $\\chi$ is given by $$\\mathcal{L}_{KL}\\Big( P|| Q \\Big)=\\sum_{x\\in\\chi}P(x)\\log\\bigg(\\frac{P(x)}{Q(x)}\\bigg)=\\sum_{x\\in\\chi}P(x)\\bigg(\\log(P(x))-\\log(Q(x))\\bigg)$$ where $P$ is the reference distribution.  \n",
    "In this case, we will take the one-hot encoded target values as the distribution $P$, and our softmaxed network output as the distribution $Q$.  \n",
    "Using $\\sigma$ for the softmax function on $\\vec{y}^{pred}$, we have \n",
    "$$\\mathcal{L}_{KL}=\\sum_{i} y^{(targ)}_i\\bigg(\\log\\big(y^{(targ)}_i\\big)-\\log(\\vec{\\sigma}_i)\\bigg)$$  \n",
    "$$\\vec{\\delta_1}=\\frac{\\partial \\mathcal{L}_{KL}}{\\partial \\vec{y}^{pred}}=\\frac{\\partial \\mathcal{L}_{KL}}{\\partial \\sigma}\\frac{\\partial \\sigma}{\\partial \\vec{y}^{pred}}$$  \n",
    "and the vector derivative of the softmax function is\n",
    "$$\\frac{\\partial \\vec{\\sigma}}{\\partial \\vec{y}^{pred}}\\bigg|_{i,j}=\\begin{cases} \\frac{\\Bigg(\\bigg(\\sum_k \\exp(y^{pred}_k)\\bigg)-\\exp(y^{pred}_{i})\\Bigg)\\exp(y^{pred}_{i})}{\\bigg(\\sum_k \\exp(y^{pred}_k)\\bigg)^2} &\\mbox{if } i=j \\\\\n",
    "\\frac{-\\exp(y^{pred}_{i})\\exp(y^{pred}_{j})}{\\bigg(\\sum_k \\exp(y^{pred}_k)\\bigg)^2} & \\mbox{if } i\\neq j \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Utility Functions\n",
    "Some useful functions for processing output.\n",
    "- Softmax: converts neural network output into a probability distribution\n",
    "- Softmax-to-one-hot: converts softmaxed data into one hot by setting the highest probability to 1 and the others to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Utility functions \"\"\"\n",
    "\n",
    "def softmax(x_array): # softmax function\n",
    "    \"\"\" Softmax function \"\"\"\n",
    "    return np.exp(x_array)/np.sum(np.exp(x_array))\n",
    "\n",
    "def softmax_derivative(x_array):\n",
    "\n",
    "    x_exp = np.exp(x_array)\n",
    "    x_sum  = np.sum(np.exp(x_array))\n",
    "\n",
    "    diag = lambda ex: (x_sum*ex-(ex**2))/(x_sum**2)\n",
    "    offdiag = lambda ex1, ex2: -(ex1*ex2)/(x_sum**2)\n",
    "    \n",
    "\n",
    "    sigma_prime_matrix = np.array([[diag(x1) if i==j\n",
    "                                    else offdiag(x1, x2)\n",
    "                                    for j, x2 in enumerate(x_exp)]\n",
    "                                   for i, x1 in enumerate(x_exp)])\n",
    "    \n",
    "    return sigma_prime_matrix\n",
    "    \n",
    "\n",
    "def softmax_to_one_hot(softmax_array): # convert softmax back to one hot\n",
    "    \"\"\" Converts softmax vector to one hot encoded array \"\"\"\n",
    "    output = np.zeros(len(softmax_array), dtype='int')\n",
    "    output[np.argmax(softmax_array)]=1\n",
    "    return output\n",
    "\n",
    "\n",
    "def train_test_split(inp_data, out_data, frac=0.7, random_seed=420):\n",
    "    \"\"\" Shuffles a list of indices and returns split, randomized data \"\"\"\n",
    "    assert(len(inp_data)==len(out_data))\n",
    "    \n",
    "    idxs = np.arange(len(inp_data)) # generate list of indices\n",
    "    random.Random(random_seed).shuffle(idxs) # randomly permute the list\n",
    "\n",
    "    split_idx = round(len(inp_data)*frac) # get the first frac of the indices\n",
    "    \n",
    "    X_train, y_train = inp_data[:split_idx], out_data[:split_idx] # split the data\n",
    "    X_test, y_test = inp_data[split_idx:], out_data[split_idx:]    # hold the test set\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Multilayer Perceptron Classes\n",
    "We define a Node class to provide a neuron API for use by the network.  \n",
    "The node is supplied with an activation function and input dimensions, and has a method for evaluating the function over the inputs, and updating the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\" Generic Node class \"\"\"\n",
    "    input_weights = None    \n",
    "    activation_function = None\n",
    "    \n",
    "    def __init__(self, input_dimension, activation_function):\n",
    "        self.input_weights = np.random.rand(input_dimension)\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "    def fire(self, input_vector):\n",
    "        assert(len(input_vector) == len(self.input_weights))\n",
    "        wTx = np.inner(self.input_weights, input_vector)\n",
    "        return self.activation_function(wTx)\n",
    "\n",
    "    def update_weights(self, new_weights):\n",
    "        assert(new_weights.size == self.input_weights.size)\n",
    "        self.input_weights = new_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The MLPNetwork class composes nodes into a network and provides an interface for interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MLPNetwork:\n",
    "    \"\"\" Class that composes Nodes into a NN and provides interface \"\"\"\n",
    "    num_hidden_nodes = None\n",
    "    activation_function = None\n",
    "    loss_function = None\n",
    "    \n",
    "    hidden_layer = None\n",
    "    output_layer = None\n",
    "\n",
    "    hidden_output = None\n",
    "    network_output = None\n",
    "\n",
    "    learning_rate = None\n",
    "    \n",
    "    network_created = False\n",
    "    verbose = None\n",
    "    \n",
    "    def __init__(self, num_hidden_nodes, activation_function=sigmoid, loss_function='kldiv', verbose=True, learning_rate=0.1):\n",
    "        assert(num_hidden_nodes>0)\n",
    "        self.num_hidden_nodes = num_hidden_nodes\n",
    "        self.activation_function = activation_function\n",
    "        self.loss_function = loss_function\n",
    "        self.learning_rate = learning_rate\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def build_network(self, input_dimension, output_dimension):\n",
    "        \"\"\" Builds the network from parameters \"\"\"\n",
    "        self.input_dimension = input_dimension\n",
    "        self.output_dimension = output_dimension\n",
    "\n",
    "        self.hidden_layer = [Node(self.input_dimension+1, self.activation_function)\n",
    "                             for num in range(self.num_hidden_nodes)]\n",
    "\n",
    "        \"\"\" Add bias neuron to hidden layer \"\"\"\n",
    "        self.hidden_layer.append(Node(self.input_dimension+1, self.activation_function))\n",
    "        \n",
    "        self.output_layer = [Node(self.num_hidden_nodes+1, self.activation_function)\n",
    "                             for num in range(self.output_dimension)]\n",
    "\n",
    "\n",
    "        self.network_created = True\n",
    "        \n",
    "    def delete_network(self):\n",
    "        \"\"\" Deletes the network \"\"\"\n",
    "        self.hidden_layer = None\n",
    "        self.output_layer = None\n",
    "        self.network_created = False\n",
    "\n",
    "\n",
    "    def training_epoch(self, X, y_target, point_idx=(None, None)):\n",
    "        if (self.verbose and point_idx!=(None, None)):\n",
    "            this_percentage = round(100*(point_idx[0]/point_idx[1]))\n",
    "            print(\"[*] Training {0}%\".format(this_percentage), end='\\r')\n",
    "\n",
    "\n",
    "        y_pred = self.feedforward(X)\n",
    "        self.backpropagate(X, y_pred)\n",
    "\n",
    "        return y_pred\n",
    "        \n",
    "    def train(self, X_train, y_train):\n",
    "        if not self.network_created:\n",
    "            self.build_network(X_train.shape[1], y_train.shape[1])\n",
    "\n",
    "        training_set = zip(X_train, y_train)\n",
    "        total = len(X_train)\n",
    "            \n",
    "        y_preds = np.array([self.training_epoch(X, y_target, (idx, total))\n",
    "                            for idx, (X, y_target) in enumerate(training_set)])\n",
    "        \n",
    "    def read_weight_matrices(self):\n",
    "        \"\"\" Read the weights from the layer nodes into matrices \"\"\"\n",
    "\n",
    "        self.hidden_weights = np.array([hidden_node.input_weights\n",
    "                                        for hidden_node in self.hidden_layer])\n",
    "    \n",
    "    \n",
    "        self.output_weights = np.array([output_node.input_weights\n",
    "                                        for output_node in self.output_layer])\n",
    "    \n",
    "    def write_weight_matrices(self):\n",
    "        \"\"\" Write a new weight matrix back to the nodes \"\"\"\n",
    "        [self.hidden_layer[idx].update_weights(weight_vector)\n",
    "         for idx, weight_vector in enumerate(self.hidden_weights)]\n",
    "\n",
    "        [self.output_layer[idx].update_weights(weight_vector)\n",
    "         for idx, weight_vector in enumerate(self.output_weights)]\n",
    "\n",
    "        \n",
    "    def backpropagate(self, X, y):\n",
    "\n",
    "        self.read_weight_matrices() # grab current weights\n",
    "\n",
    "        if self.loss_function == 'test':\n",
    "\n",
    "            #gradient_hidden =\n",
    "            #gradient_output = \n",
    "            pass\n",
    "        else:\n",
    "            if self.loss_function == 'kldiv':\n",
    "                \"\"\" row vector (row vector right multiplies matrix) \"\"\"\n",
    "                kl_deriv = y/softmax(self.network_output)\n",
    "                sm_deriv = softmax_derivative(self.network_output)\n",
    "                \n",
    "                delta_1 = np.matmul(kl_deriv, sm_deriv)\n",
    "\n",
    "            if self.activation_function == relu:\n",
    "                \"\"\" matrix \"\"\"\n",
    "                delta_2 = np.diag(relu_derivative(self.output_weights @ self.hidden_output))\n",
    "\n",
    "            elif self.activation_function == sigmoid:\n",
    "                \"\"\" matrix \"\"\"\n",
    "                delta_2 = np.diag(self.network_output*(1-self.network_output))\n",
    "            \n",
    "            elif self.activation_function == tanh:\n",
    "                \"\"\" matrix \"\"\"\n",
    "                delta_2 = np.diag(sec2h(self.output_weights @ self.hidden_output))\n",
    "                \n",
    "            \"\"\" matrix \"\"\"\n",
    "            delta_3 = self.output_weights\n",
    "                \n",
    "            \"\"\" matrix (row vector right multiplies matrix right multiplies matrix right multiplies matrix).T right multiplies (column vector).T \"\"\"\n",
    "            gradient_hidden = np.outer((delta_1 @ delta_2 @ delta_3).T, np.append(X, 1).T)\n",
    "\n",
    "            \"\"\" matrix (row vector right multiplies matrix right multiplies matrix).T right multiplies (column vector).T \"\"\"\n",
    "            gradient_output = np.outer((delta_1 @ delta_2).T, (self.hidden_output).T)\n",
    "\n",
    "        self.hidden_weights = self.hidden_weights-self.learning_rate*gradient_hidden\n",
    "        self.output_weights = self.output_weights-self.learning_rate*gradient_output\n",
    "\n",
    "        self.write_weight_matrices()\n",
    "        \n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        \n",
    "        output_vector = [self.feedforward(X_t) for X_t in X_test] \n",
    "        y_pred = np.array([softmax_to_one_hot(softmax(output)) for output in output_vector])\n",
    "        return y_pred\n",
    "    \n",
    "    def feedforward(self, input_vector):\n",
    "        \"\"\" Propagates a single datapoint with bias through the network \"\"\"\n",
    "        bias = 1\n",
    "        self.hidden_output = np.array([hidden_node.fire(np.append(input_vector,bias)) for hidden_node in self.hidden_layer])\n",
    "        self.network_output = np.array([output_node.fire(self.hidden_output) for output_node in self.output_layer])\n",
    "\n",
    "        return self.network_output\n",
    "\n",
    "    def import_config(self, filename='./mlpconfig'):\n",
    "        config_file = pickle.load(open(filename, 'rb'))\n",
    "        \n",
    "        self.hidden_weights = config_file['h_weights']\n",
    "        self.output_weights = config_file['o_weights']\n",
    "        self.num_hidden_nodes = config_file['nhidden']\n",
    "        self.loss_function = config_file['loss']\n",
    "        \n",
    "        self.build_network(config_file['idim'], config_file['odim'])\n",
    "        self.write_weight_matrices()\n",
    "\n",
    "    def export_config(self, filename='./mlpconfig'):\n",
    "        cfg_dict = {'h_weights':self.hidden_weights,\n",
    "                    'o_weights':self.output_weights,\n",
    "                    'nhidden':self.num_hidden_nodes,\n",
    "                    'loss':self.loss_function,\n",
    "                    'idim':self.input_dimension,\n",
    "                    'odim':self.output_dimension}\n",
    "        pickle.dump(cfg_dict, open(filename, 'wb+'))\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Usage\n",
    "Following the sklearn API, a network is initialized with various parameter values.  \n",
    "The training set is loaded and split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp_net = MLPNetwork(num_hidden_nodes=100,\n",
    "                     activation_function=tanh,\n",
    "                     loss_function='kldiv',\n",
    "                     verbose=True,\n",
    "                     learning_rate=0.1)\n",
    "\n",
    "\n",
    "X_train, y_train, X_test, y_test = train_test_split(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The network is trained via the train method, which takes a training and test set as arguments.  \n",
    "Prediction is done via the predict method and takes X values as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Training 100%\r"
     ]
    }
   ],
   "source": [
    "mlp_net.train(X_train, y_train)\n",
    "y_predictions = mlp_net.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The configuration of the network can be exported with the method below, and imported by replacing export with import.  \n",
    "A filename can be supplied, but there is a default ('./mlpconfig')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp_net.export_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The following code simply checks that a model with weights loaded gives the same result as one with training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp_net2 = MLPNetwork(num_hidden_nodes=100,\n",
    "                     activation_function=tanh,\n",
    "                     loss_function='kldiv',\n",
    "                     verbose=True,\n",
    "                     learning_rate=0.1)\n",
    "mlp_net2.import_config()\n",
    "y_predictions2 = mlp_net.predict(X_test)\n",
    "print(y_predictions.shape==y_predictions2.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "name": "a1_m7lynch_q4.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
